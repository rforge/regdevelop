% -*- Mode: noweb; noweb-default-code-mode: R-mode; -*-
%\SweaveUTF8
\documentclass[11pt]{article}
\usepackage{graphicx}
%% \usepackage{Sweave}
\usepackage[utf8]{inputenc}
%% \usepackage{germanU}
%%- \usepackage[noae]{Sweave}
\usepackage[a4paper]{geometry}  %% , text={14.5cm,22cm}
\usepackage{color} %uncomment BF
\usepackage{booktabs} % nice tables with \toprule \middlerule \bottomrule
\usepackage{amsmath} % for align
% \usepackage{wasysym} % for promille sign
% \usepackage{amssymb}
% \usepackage[textfont=it,font=small,labelfont=it]{caption}
\interfootnotelinepenalty=10000 % prevent LaTex from two-sided footnotes
\usepackage{regr-desc}
%\VignetteEngine{knitr::knitr}
%\VignetteDepends{regr}
%\VignetteIndexEntry{'regr: Augmented Regression Analysis'}

\addtolength{\textwidth}{2.5cm}%%--- 15.0 + 2.5 = 17.5
\addtolength{\oddsidemargin}{-1.04cm}

%% ================================================================

\begin{document}
%% \bibliography{regrbib}
%% \SweaveOpts{concordance=TRUE,width=9,height=6, echo=false}
\setkeys{Gin}{width=0.95\textwidth}
\baselineskip 15pt

\title{\vspace*{-10mm}
Package \T{regr} for an Augmented 
Regression Analysis}
\author{Werner A. Stahel, ETH Zurich}
\maketitle

\begin{abstract}\noindent
The R function \T{regr} is a wrapper function that allows for fitting
several different types of regression models by a unified call, and
provides more informative numerical and graphical output than the 
traditional \T{summary} and \T{plot} methods.
The package \T{regr} contains the functions that go along with 
\T{regr} and some more that help develop regression models.
%%- It is written to make data analysis more effective by providing
%%- user-oriented, flexible functions.
It is available from \T{R-forge} and is still in development.
\end{abstract}

<<preliminary, echo=F>>=
## library(plgraphics, lib.loc="/u/stahel/R/regdevelop/pkg/plgraphics.Rcheck")
library(regr) ##, lib.loc="/u/stahel/R/regdevelop/pkg/regr.Rcheck")
plmframes()
options(project="regr documentation")
options(warn=1)
@ 
\section{Introduction}

Regression models are fitted in the statistical programming environment R 
by diverse functions, depending on the particular type of target variable.
Outputs obtained by calling the function \T{summary} on the object produced
by the fitting function look often quite similar. Graphical output for
residual analysis is obtained by calling \T{plot}, but the result is not
always informative. 

The function \T{regr} allows for fitting various regression models
with the same arguments.
The result contains more informative numerical output than the calls to
standard R functions.
Residual analysis obtained by \T{plot}ing the result produces more appropriate
and enhanced displays through the functions available from the package
\T{plgraphics}.

\T{regr} proceeds by 
checking arguments, then calling the suitable fitting method from standard
R or other packages,
collecting useful statistics from the resulting object and a call of 
\T{summary} on it and adding a few to generate an object of class
\T{regr}. 
%%- The printing method for these objects shows the results that are usually 
%%- of interest.
%%- The plotting method produces a much more complete set of residual plots
%%- than the plotting methods for the usual fitting objects do.


In particular, the following models can be fitted by \T{regr}:
\begin{itemize}
\item 
  ordinary linear models, using Least Squares or robust estimation,
  by calling \T{lm} or \T{lmrob} from the \T{robustbase} package,
\item
  generalized linear models, by calling \T{glm},
\item
  multinomial response models, by calling \T{multinom} of package
  \T{nnet},
\item
  ordered response models, by calling \T{polr} of package
  \T{MASS},
\item
  models for survival data and Tobit regression, by calling
  \T{survreg} or \T{coxph} of package \T{survival},
\item
  multivariate ordinary linear models, by calling \T{lm},
\item
  nonlinear models, by calling \T{nls}.
\end{itemize}

This document presents the main features of the package \T{regr}
and explains the ideas behind them. 
It gives no details about the functions. They can be found in
the help files.

The package is available from \T{R-forge}, e.g. by calling\\
\T{install.packages("regr", repos="http://r-forge.r-project.org")}.\\
The reason why it is not on CRAN is that the author is still 
developing additional features and
does not yet want to guarantee upward compatibility.
It also means that comments and suggestions are very welcome:
\T{stahel-at-stat.math.ethz.ch} %% !!! at

\section{Numerical Output}
The useful numerical output of fitting functions is usually obtained by
calling \T{summary} on the object produced by the fitting method.
This results, for most functions, in a table showing the estimated
regression coefficients, their standard errors, the value of a test
statistic (t or z or deviance) and, for the 
ordinary linear model, a p value for the tests for zero coefficients. 
It is followed by an overall summary, usually including a test for
the hypothesis that all coefficients are zero, and a standard deviation of
the error and coefficient of determination, if applicable.

If there are factors (qualitative explanatory variables) in the model, 
the coefficients are not always interpreted adequately, and the 
respective standard errors, t and p values are of little use and often
misinterpreted. 
On the other hand, the information whether a factor has a significant
effect is not available from the summary but has to be obtained by calling 
\T{drop1} on the fit object. 
(The function \T{anova}, which seems to be suited according to its
name, usually does not answer this question.)

This situation cannot be called user friendly.
The function \T{regr} is meant to provide the results that are needed
without having the user call different functions and select the 
output that is safe to be interpreted.

Here is a result of printing a \T{regr} object.
<<regrout>>=
data(d.blast, package="plgraphics")
attr(d.blast,"doc")[1] <- 
  "Blasting: measurements of tremor in nearby house basements"
regroptions(printstyle="conventional")
r.blast <- regr(logst(tremor) ~ location + log10(distance) + log10(charge), 
    data = d.blast)
r.blast

@ 
The package also implements a new alternative set of characteristics for the
terms in the model, including ``relevance'' measures, see
%%\citeasnoun{StaW20} 
Stahel (2020) and below.
As shown in the previous and the following R statements, the selection of the
desired output is done by setting an option, using the function 
\T{regroptions} to be introduced below.
<<regroutrel>>=
regroptions(printstyle="relevance")
r.blast

@ 

\subsection{Standard output for continuous explanatory variables}
The \T{Terms:} table characterizes the effects of the individual terms in
the model. For continuous explanatory variables (the last 2 lines in the
example) it shows, when the ``conventional'' columns are selected,
\begin{description}
\item[\T{coef},] the estimated value of the coefficient;
\item[\T{df},] degrees of freedom, $=1$ for continuous variables;
\item[\T{ciLow, ciHigh},] the limits of the confidence interval;
\item[\T{R2.x},] the coefficient of determination for regressing 
  the explanatory variable in question on the other terms in the model.
  This is one of the wellknown collinearity diagnostics.
\item[\T{signif0},] a significance measure that is $>1$ for estimated
  coefficients differing significantly from 0, see below for its
  definition;
\item[\T{p.value},] the p value for testing if the coefficient could be
  zero; the usual significance symbols are attached.
\end{description}
With the ``relevance'' selection, the output shows
\begin{description}
\item[\T{coef, df, R2.x},] as above;
\item[\T{coefRle},] the estimated Relevance of the coefficient;
\item[\T{coefRls},] the secured Relevance of the coefficient;
\item[\T{coefRlp},] the potential Relevance of the coefficient;
  \T{Rls} and \T{Rlp} form a confidence interval for relevance;
\item[\T{dropRls},] the secured Relevance of the term, 
  see Section \ref{sec:factors}.
\end{description}

\Tit{Significance.}
The usual \T{summary} output of fitting functions includes the 
t (or z) values of the coefficients as a column in the coefficients 
table. 
They are simply the ratios of the two preceding columns in the standard output.
Nevertheless, they provide a kind of strength of the significance of the
coefficients. The p value may also serve as such a measure, but it is less 
intuitive as it turns tiny for important variables, making comparisons
somewhat more difficult than t values. 
The significance of t values depends on the number of degrees of freedom, but
informed users will know that critical values are around $\pm2$, and they will
therefore informally compare t values to $\pm2$. 
Based on these considerations, we introduce a new measure of significance
here (see %\cite{StaW20}
Stahel, 2020).

The new significance measure is defined as
\[
  \T{signif0} = \T{t value}\;/\; \T{critical value}
\]
where \T{critical value} is the critical value $q_{df}$ of the 
t distribution and depends on the degrees of freedom of the residuals. 
%%- The definition is applicable for continuous explanatory
%%- variables as well as for binary factors.
%%- For other factors, we will extend this definition below.

%%- \Tit{A note on coefficients.}
%%- Note, however, that increasing one $X^{(j)}$ without also changing
%%- others is usually a natural step in experimental applications but
%%- may not be possible in applications with observational explanatory variables,
%%- Therefore, an interpretation of coefficients can be tricky.

%% --------------------------------------------
\Tit{Standardized Coefficients.}
Standardized coefficients are meant to allow for a comparison of the 
importance of explanatory variables that have different variances.
Each of them shows the effect on the response of increasing ``its'' 
carrier $X^{(j)}$ by one standard deviation, as a multiple 
of either the response's standard deviation or the error standard
deviation \T{sigma}, called \T{stcoef} and \T{estcoef}, respectively.
Whereas the first version is popular, the second has a more satisfactory
interpretation (see Stahel, 2020).

Note that for binary input variables, increasing the variable by one
standard deviation is impossible, since an increase can only occur from 0
to 1, and therefore, the standardized coeffients are somewhat 
counter-intuitive in this case.

\Tit{Relevance.}
In the standard situation of comparing two groups, a popular 
standarized effect size is Cohen's $d$, defined as the ratio of the 
difference between the groups to the standard deviation within groups.
The value of 0.5 has is judged as a ``medium effect size''. 
It is used as a threshold for declaring a ``relevant effect'' by
Stahel (2020).
If the two groups model is expressed as the simple regression model
with just a binary explanatory variable, the error standardized 
coefficient is $d/2$ in case of equal group sizes, and therefore,
the relevance threshold is $0.25$. 
Its use is advocated for the error standardized coefficient in general.
The relevance measure is defined as the ratio of the estimated
error standardized coefficient to this threshold, 
\T{Rle = estcoef/0.25}.

\Tit{Confidence Intervals.}
The standard errors provided by the usual \T{summary} tables allow for
calculating confidence intervals for continuous explanatory variables,
by the formula $\T{coef} \;\pm\; q_{df}\cdot\T{std.error}$.

They can be calculated alternatively as
\[
  \T{coef}\cdot\;(1\pm\;1/\T{signif0}\;)
\;.\]
This is slightly more complicated for a calculation in the mind than
\T{coef}$\,\pm\, 2\,$\T{se}, 
but the formula shows an additional interpretation of \T{signif} 
in terms of the confidence interval:
If the input variable were scaled such that the confidence interval had 
half width 1, then the estimate would be \T{signif} units away from zero.

Since the standardized coefficients are proportional to the unstandardized
one, confidence intervals are easily transfered to them.
The two default \T{regr} outputs show either the interval for \T{coef}
in columns \T{ciLow} and \T{ciUp} 
or for the relevance, as \T{Rls}, ``secured relevance,'' and 
\T{Rlp}, ``potential relevance''.

\Tit{Customized output.}
The result of the \T{regr} function stores all of these informations 
in the \T{termtable} component.
The user can select the columns to be shown by setting the 
\T{printstyle} or \T{termcolumns} options.
Asking for 
\T{regroptions(printstyle="relevance")} (the default)
or \\ %% \Hneed{100mm}
\T{regroptions(printstyle="conventional")} selects one of two
default sets of columns.

<<termtable>>=
names(r.blast$termtable)
regroptions(termcolumns=c("coef", "stcoef", "df", "signif0", "p.symbol"))

print(r.blast, call=FALSE, termeffects=FALSE) ## avoid repetition of these parts
regroptions(default="all")  ## reset the options
@ 

The columns of the \T{termtable} not yet mentioned are:
\begin{description}
\item[\T{se},] the standard errors of the estimated coefficients; 
\item[\T{testst},] the test statistic used for testing if the coefficient 
  could be zero; 
\item[\T{stcoef},] the estimated standardized coefficient,
  defined as \T{coef} times the standard deviation of the explanatory
  variable, divided by the standard deviation of the response (if
  the response is continuous as assumed here), see below for its use;
\item[\T{estcoef},] the ``error standardized'' coefficient, for which
  the error standard deviation $\sigma$ is used instead of the 
  standard deviation of the response;
\item[\T{stciLow, stciUp, estciLow, estciUp},] the respective confidence
  intervals;
\item[\T{dropRle, dropRlp},] estimated and potential Relevance for the term;
\item[\T{p.symbol, coefRls.symbol, dropRls.symbol},]
  the symbols for significance and secured relevance that are attached to
  their numerical values when printed. 
\end{description}

\subsection{Factors}
\label{sec:factors}
For factors with more than two levels, (\T{location} in the example), there
are several coefficients to be estimated. 
Their values depend on the scheme for generating the 
dummy variables characterizing the factor, which is determined 
by the \T{contrasts} option (or argument) in use.
We come back to this point below (``Contrasts'').

Note that for factors with only two levels, the problem does not arise,
since the single 
coefficient can be interpreted in the straightforward manner as 
for continuous explanatory variables. \T{regr} therefore treats binary
factors in the same way as continuous explanatory variables.

The test performed for factors with more than two levels
(or more generally for any term in the regression model), 
which is shown in the \T{Terms:} table by the \T{p.value} entry, 
is the F test for the whole factor (hypothesis: all coefficients are 0). 
It is obtained by calling \T{drop1}.
The significance measure is defined as 
\[
  \T{signif0} = \sqrt{\T{F value}\;/\;\T{critical value}}
\]
where \T{critical value} is the critical value of the F distribution.
It reduces to the former one for binary factors, up to the missing sign.

Similarly, the \textbf{relevance} is defined by comparing the full model
with the one resulting from dropping the term in question.
If $\sigma_f$ and $\sigma_r$ are the error standard deviations in the 
two models, then 
\[
  \T{Effect size} = \log(\sigma_r/\sigma_f) = 
  {\textstyle\frac12}\;
  \log\fn{(1-\T{r.squared}_f)\left/(1-\T{r.squared}_r)\right.}
\;,
\]
and \T{Relevance} = \T{Effect size} / \T{Relevance threshold} with
\T{Relevance threshold = 0.03} by default.
Its estimated value and confidence interval is contained in the columns
\T{dropRle} and \T{dropRls, dropRlp}, respectively. 
For details, see Stahel (2020).

The collinearity measure \T{R2.x} for factors is a formal generalization of 
\T{R2.x} for terms with one degree of freedom, determined by applying
the relationship with the ``variance inflation factor'',
\T{R2.x}$=1/(1-\mbox{vif})$ to the generalized vif. 
[More explanation planned.]

\Tit{All coefficients for factors.}
The usual contrast option \T{contrasts=\penalty-100%
"contr.treatment"} gives the coefficients of the dummy variables 
a clear interpretation: 
they estimate the difference of the response between level $k$ and 
level 1 for $k>1$.
Another popular setting is \T{contrasts=\penalty-100%
"contr.sum"}, for which the $k$th coefficient estimates the effect 
of the $k$th level in such a way that the sum of all coefficients is 0.
For this setting, the last of these effects is not given in the
vector of coefficients, \T{coefficients(r.blast)}.

In order to avoid ambiguities, the \T{regr} output lists the 
estimated effects for all levels of the factors after the term table.
Even though the interpretation of significance and confidence intervals of
the individual effects of the levels is delicate, \T{regr} objects include
a full table, similar to the term table explained above, for each factor.
By default, however, only the estimated effects are shown, together with the
``star symbols'' for their significance.

\Tit{Contrasts.}
An advantage of \T{contr.sum} over the usual \T{contr.treatment}
contrasts is that it avoids the (often unconscientious) choice of a
reference level -- the first level -- and allows, for each level, 
to assess immediately how large its effect is as compared to an overall
average effect.
An even more important advantage appears when interactions between factors
are included in the model: 
The main effects of one factor, including its significance, 
may still be interpreted (with caution) as average effects over the
levels of the other factor.

The \T{contr.sum} setting is not well adapted to unbalanced factors, since
the unweighted sum of coefficients is forced to be 0.
This leads to large standard errors when one of the levels has a low 
frequency. 
The \T{regr} package provides the option \T{contr.wsum} for which the
sum of coefficients weighted with the frequencies of the levels is zero.
This type of contrasts is the default in \T{regr}.

\subsection{Model summary}
The last paragraph of the output gives the summary statistics.
For ordinary linear models, the estimated standard deviation or the error
term is given first. (It is labelled ``Standard error of residual'' in the 
\T{lm} output, which we would call a blunt misnomer.)
The \T{Multiple R\^{}2} is given next, together with its ``adjusted''
version, followed by the overall F test for the model.

For generalized linear models, the deviance test for the model is given.
If applicable, a test for overdispersion based on residual deviance is also
added. 

\section{Model Development}
The functions \T{drop1} and \T{add1} are basic tools for model development.
Their method for \T{regr} objects provide additional features. 

First, they calculate tests by default.
Second, \T{add1} has a useful default value for the argument \T{scope}:
It consists of all squared continuous variables and all interactions between 
variables in the model. 
(These terms are calculated by \T{terms2order}.)

Since \T{drop1} and \T{add1} methods are available, \T{step} can be used to
select a model automatically. 
However, it is preferable to use the version \T{step.regr} instead.
It chooses ``both'' directions, backward and forward, by default, and 
extends the \T{scope} as in \T{add1}.
There are two more features that are explained in the following.

<<add1>>=
add1(r.blast)

r.step <- step.regr(r.blast, k=4)
formula(r.step)
@ 

\Tit{Stopping rule.}
The usual stopping rule for stepwise model selection relies on the AIC
criterion. Note that there is a funny argument for this choice:
The rule that was commonly used early on stopped when all terms were
formally significant. It has been criticized because of the 
multiplicity problem: The retained terms appear more significant than they
are, because they are the most significant of an larger number of
candidates. Thus, the formal significance tests for the terms are liberal
to an unknown degree, and the selected models tend to be too large.
Therefore, a new criterion was introduced, the AIC (or the BIC).
Its justification is based on optimization of a prediction error sum of 
squares. It should be noted that it leads to even larger models than the
significance criterion. 
%%- Thus, the criterion now commonly used leads
%%- Thus, one should expect to see non-significant
%%- terms in the selected model, even based on the liberal formal tests
%%- provided in regression outputs.

Unfortunately, it is not possible to specify the stopping criterion 
in \T{step}. However, setting its argument \T{k=4} --
%% was k=5
the default for \T{step.regr} --, 
one usually obtains a model with formally significant terms, for which
every additional regressor would be non-signifcant.
\\
($\mbox{AIC}\sups k$ is $2L+k\mbox{df}$, where $L$ is the log likelihood.
When dropping a single regressor from model 1 to obtain model 2,
$2(L_1-L_2)$ has a chisquared distribution with 1 degree of freedom,
for which the critical value is $3.84$.
Thus, $\mbox{AIC}_2\sups{3.84}-\mbox{AIC}_1\sups{3.84}<0$,
if and only if $2(L_1-L_2)<3.84$, i.e., the test is non-significant, 
and the regressor will be dropped.)

\Tit{Missing values.}
The \T{regr} methods for \T{drop1} and \T{step} behave more flexibly than
in basic \T{R} when there are missing values: \\
\T{drop1} drops the rows in 
\T{data} that contain a missing value for the regressors in the current
model and proceeds, rather than complaining about a variable number of
missing observations.\\
\T{add1} drops the rows with missings in any variable used by the
\T{scope} before calculations.\\
\T{step.regr} uses these two functions for the two directions.
It only uses the observations without missing values in any variables
in \T{scope}, but gives as the resulting fit the one based on the 
complete observations for the variables in the final formula.

\subsection{Model Comparisons}
When model development is part of the statistical analysis, it is useful to
compare the terms that occur in different models under consideration.
There is a function called \T{modelTable} that collects coefficients, 
p values, and other useful information, and a \T{format} and \T{print} 
method for showing the information in a useful way.

<<modeltable>>=
r.blast2 <- 
  regr(logst(tremor) ~ ( location + log10(distance) + log10(charge) )^2,
       data=d.blast)
modelTable(c("r.blast", "r.blast2"))             
@ 

\section{Residual Analysis}

Various graphical displays involving residuals are of great help to
check if assumptions are violated and to find improved models.
Plain R provides four displays when the plot function is called
for \T{lm} and similar objects.
When \T{plot} is called on \T{regr} objects, it produces enhanced displays 
that are adapted to the type of fitted model and adds plots of residuals
against input variables.
This is implemented by making the function \T{plregr} in the package
\T{plgraphics} the plot method for class \T{regr}.
The \T{plgraphics} package features easy choices of most graphical
elements, like plotting symbols, colors, tickmarks, etc.
It implements methods for displaying censored variables, residuals from 
ordinal regression, plotting residuals against two variables, and more.
See the package vignette and help files for details.

Here are the residual plots shown by default.

!!! No, this currently produces an error. Please see the vignette of 
\T{plgraphics}
%%- <<resanal, fig.height=8, fig.width=9>>=
%%- plot(r.blast)
%%- @ 
%%- %\pagebreak
%%- \Vneed{50mm}

%% \section{Other regression models}

\section{Details}
\subsection{regr options}
Regr options shape the output of the \T{regr} function or even influence
the way fitting functions are called.

Here is a list of the options.
\begin{description}
\item[digits]          number of digits used when printing \T{regr} objects;
\item[regr.contrasts]  contrasts used as a default in generating designs;
\item[factorNA]        logical: for explanatory factors, should the 
  missing values form a level of the factor?
\item[testlevel]       level of tests = 1 - confidence level (two sided);
\item[RlvThres]        vector of length 2: thresholds for relevance of 
  coefficients and terms (component \T{drop});
\item[termtable]       logical: should a term table be calculated?
\item[vif]
\item[show.termeffects] logical: should term effects be shown?
\item[termcolumns]     names of columns of the \T{termtable} component 
  of the fitting result object which will be printed;
  alternatively, 
\item[printstyle]   can be set either to \T{"relevance"} or to \T{"conventional"}:
  these then select the first or second of the following components;
\item[termcolumns.r, termcolumns.c] columns selected in the case just mentioned;
\item[termeffcolumns] same for the term effects tables;
\item[coefcolumns]    same for the coefficients' table for each term;
\item[na.print]       symbol by which NA's are printed;
\item[notices]        logical: should notices be shown?
\end{description}

\subsection*{References}

Stahel, Werner A. (2020). 
\textit{Measuring Significance and Relevance instead of p-values.}
In preparation.

\vspace{20mm}
{\small
\Tit{\noindent This is the end} 
of the story for the time being, \the\day.\the\month.\the\year. 
More material will be added for details about the many more 
regression models that are supported.\\
I hope that you will get into using \T{regr} and have 
good success with your data analyses.
Feedback is highly appreciated.

\noindent
Werner Stahel, \T{stahel at stat.math.ethz.ch}
}
\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
